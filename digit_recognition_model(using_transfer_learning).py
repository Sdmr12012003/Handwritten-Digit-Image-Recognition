# -*- coding: utf-8 -*-
"""Digit_Recognition_Model(Using Transfer Learning).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1trMMIyyqMC-ZuMixWTAwyESsbwCgxAYc
"""

import datetime
from tensorflow import keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import backend as K

now = datetime.datetime.now

now()

#HYPERPARAMETERS INITIALISATION

batch_size = 128
num_classes = 5
epochs = 5

#PARAMETERS INITIALISATION FOR CNN-ARCHITECTURE

img_rows, img_cols = 28,28
filters = 32
pool_size = 2
kernel_size = 3

#THIS HANDLES SOME VARIABILITY IN HOW INPUT DATA IS LOCATED

if K.image_data_format() == 'channels_first':
  input_shape = (1 , img_rows,img_cols)
else:
  input_shape = (img_rows,img_cols , 1)

#to simplify things, a function to include all the training steps
## As input, function takes a model, training set, test set, and the number of classes
## Inside the model object will be the state about which layers we are freezing and which are training or fine-tuning the layers

def train_model(model , train, test , num_classes):
  x_train = train[0].reshape((train[0].shape[0],) + input_shape)
  x_test = test[0].reshape((test[0].shape[0] ,) + input_shape)
  x_train = x_train.astype("float32")
  x_test = x_test.astype('float32')
  x_train /= 255 # scalingg every pixel value less than 1 and gaeter than 0
  x_test /= 255
  print("x_train shape:" , x_train.shape)
  print(x_train.shape[0] , 'train samples')
  print(x_test.shape[0] , 'test samples')

  # convert class vectors into binary classification or one-hot encoding

  y_train = keras.utils.to_categorical(train[1] , num_classes)
  y_test = keras.utils.to_categorical(test[1] , num_classes)

  model.compile(loss = 'categorical_crossentropy' , optimizer = 'adadelta' , metrics = ["accuracy"]) # here adaadelta used instead of Adam or RMSprop to slow or smoothen process of finding optimal weights

  t = now()
  model.fit(x_train , y_train , batch_size = batch_size , epochs =  epochs , verbose = 1 , validation_data = (x_test,y_test) )
  print("Training time is " , (now() - t))

  score = model.evaluate(x_test , y_test , verbose = 1)
  print("Test score : " , score[0])
  print("Test accuracy : " , score[1])

(x_train, y_train), (x_test, y_test) = mnist.load_data()

#create two datasets: one with digits below 5 and one with 5 and above

x_train_lt5 = x_train[y_train < 5]
y_train_lt5 = y_train[y_train < 5]
x_test_lt5 = x_test[y_test < 5]
y_test_lt5 = y_test[y_test < 5]

x_train_gt5 = x_train[y_train >= 5]
y_train_gt5 = y_train[y_train >= 5] - 5
x_test_gt5 = x_test[y_test >= 5]
y_test_gt5 = y_test[y_test >= 5] - 5

#defining a "early features layers" which are freezed from fine-tuning and will be
 #used for Transfer Learning for other model's architecture

feature_layers = [
    Conv2D(filters , kernel_size , padding = 'valid' , input_shape = input_shape),
    Activation('relu'),
    Conv2D(filters, kernel_size),
    Activation('relu'),
    MaxPooling2D(pool_size = pool_size),
    Dropout(0.25),
    Flatten(),
]

# defining fine-tuning layers or outer_domain of CNN layers

classification_layers = [
    Dense(128),
    Activation('relu'),
    Dropout(0.5),
    Dense(num_classes),
    Activation('softmax')
]

# creating sequential models by combining two above layers into single

model = Sequential(feature_layers + classification_layers)

model.summary()

#training the model to identify digits 5,6,7,8,9

train_model(model,
            (x_train_gt5 , y_train_gt5),
            (x_test_gt5 , y_test_gt5), num_classes)

#FREEZING EARLY LAYERS

for i in feature_layers:
  i.trainable = False

model.summary() #here training params less than earlier due to save of traing of early layers which is not neede for next task

train_model(model , (x_train_lt5,y_train_lt5),(x_test_lt5,y_test_lt5), num_classes)

